# api_types.py
from __future__ import annotations # Ensure forward references work smoothly

from collections.abc import Iterable
from functools import cached_property
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Type, TypeAlias, Union # Added TypeAlias
import datetime # Added datetime for timestamp
import logging # Added logging

# Ensure FasterWhisper types are importable or define fallbacks if optional
try:
    import faster_whisper.transcribe
    # Create type aliases when import succeeds
    FasterWhisperSegment: TypeAlias = faster_whisper.transcribe.Segment
    FasterWhisperTranscriptionInfo: TypeAlias = faster_whisper.transcribe.TranscriptionInfo
except ImportError:
    # Define dummy types if FasterWhisper is optional or for type checking without install
    logging.getLogger(__name__).warning("faster_whisper not found, using dummy types for TranscriptionInfo/Segment.")
    class TranscriptionInfo: # Dummy class
         language: str = "unknown"
         duration: float = 0.0
         duration_after_vad: Optional[float] = None
         options: Any = None # Placeholder for options
         vad_options: Any = None # Placeholder
         transcription_options: Any = None # Placeholder

    class FasterWhisperSegment: # Dummy class
         id: int = 0
         seek: int = 0
         start: float = 0.0
         end: float = 0.0
         text: str = ""
         tokens: List[int] = []
         temperature: float = 0.0
         avg_logprob: float = 0.0
         compression_ratio: float = 0.0
         no_speech_prob: float = 0.0
         words: Optional[List[Any]] = None # Placeholder for word type
    
    # Create type aliases for the dummy types
    FasterWhisperTranscriptionInfo: TypeAlias = TranscriptionInfo
    # FasterWhisperSegment is already defined

    # We can still create the runtime dummy for compatibility
    _faster_whisper_dummy = type('FasterWhisperDummy', (object,), {'transcribe': type('TranscribeDummy', (object,), {'TranscriptionInfo': TranscriptionInfo, 'Segment': FasterWhisperSegment})})()
    
from pydantic import BaseModel, Field, computed_field, field_validator, HttpUrl

# Assuming text_utils is correctly placed and importable
try:
    # Ensure the path is correct relative to this file or project structure
    from text_utils import segments_to_text # Example relative import
except ImportError:
     try:
         from text_utils import segments_to_text # Try absolute import
     except ImportError:
          # Basic placeholder if text_utils is missing during type checking
          def segments_to_text(segments): return ""
          logging.getLogger(__name__).warning("`text_utils` not found, using placeholder for segments_to_text.")


# --- Constants ---
# Moved from config.py for better type definition location
ModelTask = Literal["automatic-speech-recognition", "text-to-speech", "voice-activity-detection"]
ResponseFormat = Literal["mp3", "flac", "wav", "pcm"] # Formats generated by *this* API's TTS

SUPPORTED_RESPONSE_FORMATS: tuple[ResponseFormat, ...] = ("mp3", "flac", "wav", "pcm")
DEFAULT_RESPONSE_FORMAT: ResponseFormat = "mp3"


# --- Base Model for API Responses ---
class BaseApiModel(BaseModel):
     object: str = Field(..., description="The type of object returned (e.g., 'list', 'model').")

# --- Transcription Related Models ---

class TranscriptionWord(BaseModel):
    """Represents a single transcribed word with timing and probability."""
    start: float = Field(..., ge=0.0, description="Start time of the word in seconds from the beginning of the audio.")
    end: float = Field(..., ge=0.0, description="End time of the word in seconds from the beginning of the audio.")
    word: str = Field(..., description="The transcribed word itself.")
    probability: float = Field(..., ge=0.0, le=1.0, description="Confidence score of the transcription for this word (0.0 to 1.0).")

    @field_validator('end')
    @classmethod
    def check_end_time(cls, end_time: float, values: Any) -> float:
        """Ensures end time is not before start time."""
        # Use values.data for Pydantic v2 field access in validators
        start_time = values.data.get('start')
        if start_time is not None and end_time < start_time:
            raise ValueError('Word end time cannot be before start time.')
        return end_time

    @classmethod
    def from_segments(cls, segments: Iterable["TranscriptionSegment"]) -> List["TranscriptionWord"]:
        """Extracts all word objects from an iterable of transcription segments."""
        words: List[TranscriptionWord] = []
        for segment in segments:
            if segment.words: # Check if the 'words' attribute exists and is not None
                words.extend(segment.words)
            else:
                 # Log or handle cases where word timestamps might be expected but missing
                 # logger.debug(f"Segment {segment.id} has no word timestamps.")
                 pass
        return words

    def offset(self, seconds: float) -> None:
        """Adjusts the start and end times of the word by a given offset in seconds."""
        self.start = max(0.0, self.start + seconds) # Ensure time doesn't become negative
        self.end = max(self.start, self.end + seconds) # Ensure end >= start

class TranscriptionSegment(BaseModel):
    """Represents a segment of transcribed audio with timing and metadata."""
    id: int = Field(..., description="Sequential identifier for the segment within the transcription.")
    seek: int = Field(..., description="Seek offset of the segment in the audio stream.")
    start: float = Field(..., ge=0.0, description="Start time of the segment in seconds.")
    end: float = Field(..., ge=0.0, description="End time of the segment in seconds.")
    text: str = Field(..., description="The transcribed text content of the segment.")
    tokens: List[int] = Field(..., description="List of token IDs generated by the model for this segment.")
    temperature: float = Field(..., description="Temperature setting used during transcription sampling.")
    avg_logprob: float = Field(..., description="Average log probability of the tokens in the segment.")
    compression_ratio: float = Field(..., description="Gzip compression ratio of the segment's text.")
    no_speech_prob: float = Field(..., ge=0.0, le=1.0, description="Probability that this segment does not contain speech.")
    words: Optional[List[TranscriptionWord]] = Field(None, description="Detailed word-level transcription with timestamps (if requested).")

    @field_validator('end')
    @classmethod
    def check_end_time(cls, end_time: float, values: Any) -> float:
        """Ensures segment end time is not before start time."""
        start_time = values.data.get('start')
        if start_time is not None and end_time < start_time:
            raise ValueError('Segment end time cannot be before start time.')
        return end_time

    @classmethod
    def from_faster_whisper_segments(
        cls, segments_iterable: Iterable[faster_whisper.transcribe.Segment] # Type hint with the imported (or dummy) type
    ) -> Iterable["TranscriptionSegment"]:
        """Converts an iterable of FasterWhisper segments to Pydantic TranscriptionSegment objects."""
        for segment in segments_iterable:
            words_list = None
            # Check if segment.words exists and is not None before iterating
            if getattr(segment, 'words', None):
                 words_list = [
                     TranscriptionWord(
                         start=word.start,
                         end=word.end,
                         word=word.word,
                         probability=word.probability,
                     ) for word in segment.words # type: ignore # Assuming word has these attributes
                 ]

            yield cls(
                id=segment.id,
                seek=segment.seek,
                start=max(0.0, segment.start), # Ensure non-negative times
                end=max(segment.start, segment.end), # Ensure end >= start
                text=segment.text.strip(), # Clean up whitespace
                tokens=segment.tokens,
                temperature=segment.temperature,
                avg_logprob=segment.avg_logprob,
                compression_ratio=segment.compression_ratio,
                no_speech_prob=segment.no_speech_prob,
                words=words_list,
            )

# --- OpenAI Compatible Transcription Response Formats ---

class CreateTranscriptionResponseJson(BaseModel):
    """Standard JSON response containing only the full concatenated transcript."""
    text: str = Field(..., description="The full transcribed text.")

    @classmethod
    def from_segments(cls, segments: List[TranscriptionSegment]) -> "CreateTranscriptionResponseJson":
        """Creates response from a list of segments using the text utility."""
        return cls(text=segments_to_text(segments))

class CreateTranscriptionResponseVerboseJson(BaseModel):
    """Verbose JSON response with detailed transcription information, matching OpenAI."""
    task: Literal["transcribe", "translate"] = Field(..., description="The task performed ('transcribe' or 'translate').")
    language: str = Field(..., description="Detected or specified language code (e.g., 'en', 'es').")
    duration: float = Field(..., ge=0.0, description="Total duration of the audio processed in seconds.")
    text: str = Field(..., description="The full transcribed text.")
    # Use List for consistency with OpenAI spec, even if words are extracted from segments
    words: Optional[List[TranscriptionWord]] = Field(None, description="Word-level transcription details (if requested).")
    segments: List[TranscriptionSegment] = Field(..., description="Segment-level transcription details.")

    @classmethod
    def from_segment(
        cls, segment: TranscriptionSegment, transcription_info: faster_whisper.transcribe.TranscriptionInfo, task: Literal["transcribe", "translate"] = "transcribe"
    ) -> "CreateTranscriptionResponseVerboseJson":
        """Creates verbose response from a single segment (useful for streaming)."""
        # Calculate duration more carefully, considering VAD
        duration = transcription_info.duration_after_vad if transcription_info.vad_options else transcription_info.duration
        return cls(
            task=task,
            language=transcription_info.language,
            duration=duration, # Use appropriate duration
            text=segment.text.strip(), # Use stripped text from segment
            words=segment.words, # Include words if present on the single segment
            segments=[segment], # List contains only this segment
        )

    @classmethod
    def from_segments(
        cls, segments: List[TranscriptionSegment], transcription_info: faster_whisper.transcribe.TranscriptionInfo, task: Literal["transcribe", "translate"] = "transcribe"
    ) -> "CreateTranscriptionResponseVerboseJson":
        """Creates verbose response from a list of segments."""
        # Combine words only if word timestamps were generated based on TranscriptionInfo options
        all_words = None
        # Check if transcription_options exist and have word_timestamps=True
        # Need to adapt based on actual faster-whisper TranscriptionInfo structure
        # Placeholder check: assume if any segment has words, they were requested
        if any(s.words for s in segments):
             all_words = TranscriptionWord.from_segments(segments)

        duration = transcription_info.duration_after_vad if transcription_info.vad_options else transcription_info.duration
        return cls(
            task=task,
            language=transcription_info.language,
            duration=duration,
            text=segments_to_text(segments), # Use helper for full text
            segments=segments,
            words=all_words,
        )


# --- Model Listing Related Models ---

class Model(BaseModel):
    """Generic representation of an available model (e.g., Whisper ASR). Conforms to OpenAI's model object."""
    id: str = Field(..., description="Unique identifier for the model (e.g., Hugging Face repo ID).")
    created: int = Field(default_factory=lambda: int(datetime.datetime.now(datetime.timezone.utc).timestamp()), description="Model creation timestamp (Unix epoch seconds).")
    object: Literal["model"] = Field("model", description="Object type identifier, always 'model'.")
    owned_by: str = Field(..., description="The organization that owns the model (e.g., 'openai', 'Systran', 'SWivid', 'local').")
    # Fields added for convenience, not strictly in OpenAI spec but useful:
    language: Optional[List[str]] = Field(None, description="List of supported language codes (ISO 639-1).")
    task: ModelTask = Field(..., description="The primary task supported by the model.")

class F5TTSModel(Model):
    """Specific representation for F5-TTS/E2-TTS models, inheriting base Model fields."""
    # Override task to be specific to TTS
    task: Literal["text-to-speech"] = Field("text-to-speech", description="Task is always text-to-speech.")
    # Add architecture specific to these models
    architecture: Literal["DiT", "UNetT", "Unknown"] = Field("Unknown", description="The underlying model architecture (DiT or UNetT).")

    # Keep validator for owned_by if needed, or handle in creation helper
    # @field_validator('owned_by', mode='before')
    # @classmethod
    # def ensure_owned_by(cls, v: Optional[str], values: Any) -> str: ...


class ListModelsResponse(BaseApiModel):
    """Response model for listing available models, conforming to OpenAI."""
    object: Literal["list"] = "list"
    # IMPORTANT: Ensure this list contains objects compatible with the 'Model' schema.
    # Subclasses might work with Pydantic v2, but casting to Model is safer for strictness.
    data: List[Model] = Field(..., description="A list of available model objects.")


# --- TTS Voice Listing Model ---

class Voice(BaseModel):
    """Represents an available voice identity for TTS, simplified for listing."""
    id: str = Field(..., description="The unique identifier for the voice (e.g., 'alloy', 'my_custom_f5_voice').")
    name: Optional[str] = Field(None, description="A user-friendly name for the voice (often same as id).")
    # Optional: Add metadata about which engine/model this voice belongs to
    # Example: engine_id: Optional[Literal["openai", "f5tts", "piper"]] = None
    # Example: compatible_models: Optional[List[str]] = None

# --- Timestamp Granularity Types ---
TimestampGranularities = List[Literal["segment", "word"]]
DEFAULT_TIMESTAMP_GRANULARITIES: TimestampGranularities = ["segment"]
# Define valid combinations allowed by the API (sets are easier for comparison)
VALID_TIMESTAMP_GRANULARITY_SETS: List[set[Literal["segment", "word"]]] = [
    set(), # Equivalent to {"segment"}
    {"segment"},
    {"word"},
    {"segment", "word"},
]
TIMESTAMP_GRANULARITIES_COMBINATIONS: list[TimestampGranularities] = [
    [],  # should be treated as ["segment"]. https://platform.openai.com/docs/api-reference/audio/createTranscription#audio-createtranscription-timestamp_granularities
    ["segment"],
    ["word"],
    ["word", "segment"],
    ["segment", "word"],  # same as ["word", "segment"] but order is different
]

# --- F5-TTS Internal Request/Response Models (Not directly exposed via OpenAI API) ---
# These define the parameters used internally by the F5-TTS generation logic.

class F5TTSGenerationParams(BaseModel):
    """Internal parameters controlling the F5-TTS audio generation process."""
    speed: float = Field(1.0, ge=0.1, le=3.0)
    nfe_step: int = Field(32, ge=4, le=128)
    cross_fade_duration: float = Field(0.15, ge=0.0, le=1.0)
    remove_silence: bool = Field(False)
    output_sample_rate: Optional[int] = Field(None, ge=8000)

class F5TTSGenerationRequest(BaseModel):
    """Internal representation for an F5-TTS generation task passed to the core logic."""
    ref_audio_path: Union[str, Path] # Path on server
    gen_text: str
    ref_text: Optional[str] = None
    parameters: F5TTSGenerationParams = Field(default_factory=F5TTSGenerationParams)

class F5TTSGenerationResponseMetadata(BaseModel):
    """Internal metadata related to a completed F5-TTS generation (potentially for logging/debugging)."""
    model_id_used: str
    input_characters: int
    reference_audio_path: str
    effective_reference_text: Optional[str] = None
    output_duration_seconds: float
    original_sample_rate: int
    final_sample_rate: int
    generation_time_seconds: float
    parameters_used: F5TTSGenerationParams
    request_id: Optional[str] = None

# Removed ModifiedSpeechRequestParams as it's not used with the current endpoint structure